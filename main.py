import requests
import os
import concurrent.futures
from datetime import datetime
import time
from urllib.parse import urlparse, urlunparse

# --- è¨­å®š ---
SOURCES = {
    "http": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
    "socks4": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt",
    "socks5": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt"
}

# ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åã®å®šç¾©
FILES = {
    "http": "alive_http.txt",
    "socks4": "alive_socks4.txt",
    "socks5": "alive_socks5.txt"
}

FILE_CACHE = "list_cache.txt" # å…¨ä½“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
CHECK_URL = "http://httpbin.org/ip"
TIMEOUT = 10
MAX_WORKERS = 100 


def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")


def get_my_ip():
    try:
        return requests.get(CHECK_URL, timeout=TIMEOUT).json().get('origin')
    except Exception:
        log("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼")
        return None


def download_all_lists():
    """å…¨ãƒªã‚¹ãƒˆå–å¾—ã—ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹(socks5://ãªã©)ã‚’ä»˜ã‘ã¦çµ±åˆã‚»ãƒƒãƒˆã«ã™ã‚‹"""
    combined = set() # setã‚’ä½¿ã†ã“ã¨ã§è‡ªå‹•çš„ã«é‡è¤‡ãŒæ¶ˆãˆã¾ã™
    for proto, url in SOURCES.items():
        log(f"ğŸ“¥ {proto.upper()} ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        try:
            resp = requests.get(url, timeout=20)
            resp.raise_for_status()
            for line in resp.text.splitlines():
                p = line.strip()
                if p:
                    # çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: protocol://ip:port
                    if "://" not in p:
                        p = f"{proto}://{p}"
                    combined.add(p) # ã“ã“ã§é‡è¤‡ã¯å¼¾ã‹ã‚Œã¾ã™
        except Exception as e:
            log(f"âŒ {proto.upper()} å–å¾—å¤±æ•—: {e}")
    return combined


def load_prev_alive():
    combined = set()
    for proto, filename in FILES.items():
        if os.path.exists(filename):
            with open(filename, "r", encoding="utf-8") as f:
                for line in f:
                    p = line.strip()
                    if p:
                        combined.add(f"{proto}://{p}")
    return combined


def save_alive_split(alive_set):
    """
    ç”Ÿå­˜ãƒªã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã”ã¨ã«åˆ†ã‘ã¦ä¿å­˜ã™ã‚‹
    """
    data = {k: [] for k in FILES.keys()}

    for proxy in alive_set:
        if proxy.startswith("socks5://"):
            data["socks5"].append(proxy.replace("socks5://", ""))
        elif proxy.startswith("socks4://"):
            data["socks4"].append(proxy.replace("socks4://", ""))
        else:
            clean_ip = proxy.replace("http://", "")
            data["http"].append(clean_ip)

    # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
    for proto, filename in FILES.items():
        # â˜…ã“ã“ãŒé‡è¦: set() ã§é‡è¤‡ã‚’æ¶ˆã—ã€sorted() ã§ç¶ºéº—ã«ä¸¦ã¹ã‚‹
        unique_lines = sorted(list(set(data[proto])))
        
        with open(filename, "w", encoding="utf-8") as f:
            for line in unique_lines:
                f.write(line + "\n")
        log(f"ğŸ’¾ {filename}: {len(unique_lines)} ä»¶ ä¿å­˜")


def load_file_as_set(filename):
    if not os.path.exists(filename):
        return set()
    with open(filename, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def save_set_to_file(filename, data_set):
    with open(filename, "w", encoding="utf-8") as f:
        for item in sorted(list(data_set)):
            f.write(item + "\n")


# --- æ”¹è‰¯ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯å‘¨ã‚Š ---

def _normalize_proxy_url(proxy_url: str) -> str:
    """
    proxy_url ã‚’å—ã‘å–ã‚Šã€requests (PySocksçµŒç”±) ã§ç¢ºå®Ÿã«ä½¿ãˆã‚‹å½¢å¼ã«æ•´å½¢ã™ã‚‹ã€‚
    - socks5 -> socks5h ã«ã—ã¦ DNS ã‚’ãƒ—ãƒ­ã‚­ã‚·å´ã§è§£æ±ºã™ã‚‹ï¼ˆãƒ›ã‚¹ãƒˆåè§£æ±ºãŒå¿…è¦ãªå ´åˆã«å®‰å…¨ï¼‰
    - æ—¢ã« scheme ã‚’å«ã‚€å ´åˆã¯ãã®ã¾ã¾ä½¿ã†ãŒã€socks5 ã‚’ socks5h ã«å¤‰æ›ã™ã‚‹
    """
    if not proxy_url:
        return proxy_url
    parsed = urlparse(proxy_url)
    scheme = parsed.scheme.lower()
    netloc = parsed.netloc or parsed.path  # ä¸‡ä¸€ scheme ãŒãªã‘ã‚Œã° path ã«å…¥ã£ã¦ã„ã‚‹ã‹ã‚‚
    # æ­£è¦åŒ–: socks5 -> socks5h
    if scheme.startswith("socks5"):
        scheme = "socks5h"
    elif scheme.startswith("socks4"):
        scheme = "socks4"
    elif scheme in ("http", "https"):
        scheme = scheme
    else:
        # æœªæŒ‡å®šã‚¹ã‚­ãƒ¼ãƒ ãªã‚‰ http ã¨ã¿ãªã™
        if "://" not in proxy_url:
            return "http://" + proxy_url
    normalized = urlunparse((scheme, netloc, "", "", "", ""))
    return normalized


def _is_fast_enough(elapsed: float, timeout: int, max_allowed_ratio: float = 0.9) -> bool:
    """
    å®Ÿç”¨ä¸Šã€Œé€Ÿã„ã€ã¨ã¿ãªã™é–¾å€¤ã€‚timeout ã®å¤§éƒ¨åˆ†ã‚’è¶…ãˆã‚‹é…ã•ãªã‚‰é™¤å¤–ã™ã‚‹ã€‚
    max_allowed_ratio ã¯ timeout ã«å¯¾ã™ã‚‹æœ€å¤§è¨±å®¹æ¯”ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 90%ï¼‰
    """
    return elapsed <= timeout * max_allowed_ratio


def check_proxy(proxy_url: str, my_ip: str) -> bool:
    """
    æ”¹è‰¯ç‰ˆãƒ—ãƒ­ã‚­ã‚·ãƒã‚§ãƒƒã‚¯:
    1) proxy ã‚’æ­£è¦åŒ–ã—ã¦ requests ã«æ¸¡ã™
    2) CHECK_URL (httpbin.org/ip) ã«å¯¾ã—ã¦ GET ã‚’è¡Œã„ã€status==200, JSON ã§ origin ãŒã‚ã‚Šè‡ªåˆ†ã® IP ã‚’å«ã¾ãªã„ã“ã¨ã‚’ç¢ºèª
       â€” ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãŒ timeout ã«è¿‘ã™ãã‚‹å ´åˆã¯é™¤å¤–ï¼ˆå®Ÿç”¨æ€§ç¢ºèªï¼‰
    3) å®Ÿç”¨ãƒã‚§ãƒƒã‚¯ã¨ã—ã¦ https://httpbin.org/get ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ GET ãŒæˆåŠŸã™ã‚‹ã‹ç¢ºèªï¼ˆHTTPSçµŒè·¯ã‚„ãƒ›ã‚¹ãƒˆåè§£æ±ºã‚’ãƒã‚§ãƒƒã‚¯ï¼‰
    å¤±æ•—æ™‚ã¯ Falseã€‚å†…éƒ¨ã§ä¾‹å¤–ã¯æ¡ã‚Šã¤ã¶ã—ã¦ False ã‚’è¿”ã™ï¼ˆãƒ­ã‚°ã¯æ®‹ã™ï¼‰ã€‚
    """
    try:
        proxy_norm = _normalize_proxy_url(proxy_url)
        proxies = {"http": proxy_norm, "https": proxy_norm}

        # --- åŸºæœ¬æ¥ç¶šãƒã‚§ãƒƒã‚¯ ---
        start = time.time()
        resp = requests.get(CHECK_URL, proxies=proxies, timeout=TIMEOUT, allow_redirects=True)
        elapsed = time.time() - start

        resp.raise_for_status()
        content_type = resp.headers.get("Content-Type", "")
        if "application/json" not in content_type:
            log(f"âŒ {proxy_url} : unexpected Content-Type: {content_type}")
            return False

        data = resp.json()
        origin = data.get("origin", "")
        if not origin:
            log(f"âŒ {proxy_url} : no origin field")
            return False

        # è‡ªåˆ†ã® IP ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ç›´é€šï¼ˆãƒ—ãƒ­ã‚­ã‚·çµŒç”±ã«ãªã£ã¦ã„ãªã„ï¼‰ã¨ã¿ãªã™
        if my_ip and my_ip in origin:
            log(f"âŒ {proxy_url} : origin contains my IP ({my_ip}) -> not anonymous / not via proxy")
            return False

        # å®Ÿç”¨çš„ã«é…ã„å ´åˆã¯é™¤å¤–ï¼ˆtimeout ã«è¿‘ã„å¿œç­”ã¯å®‰å®šæ€§ã«å•é¡ŒãŒã‚ã‚‹ï¼‰
        if not _is_fast_enough(elapsed, TIMEOUT):
            log(f"âŒ {proxy_url} : too slow in basic check ({elapsed:.2f}s)")
            return False

        # --- å®Ÿç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆHTTPS, ãƒ˜ãƒƒãƒ€ç­‰ã®ç¢ºèªï¼‰---
        # ã“ã“ã§ HTTPS çµŒè·¯ã‚„ãƒ›ã‚¹ãƒˆåè§£æ±ºã€ãƒ˜ãƒƒãƒ€è¿”å´ãªã©ã®ç¢ºèªã‚’è¡Œã†
        start2 = time.time()
        resp2 = requests.get("https://httpbin.org/get", proxies=proxies, timeout=TIMEOUT)
        elapsed2 = time.time() - start2
        resp2.raise_for_status()

        # å†åº¦ JSON æ§‹é€ ã‚’ç¢ºèª
        data2 = resp2.json()
        url_field = data2.get("url", "")
        if not url_field:
            log(f"âŒ {proxy_url} : /get returned no url field")
            return False

        if not _is_fast_enough(elapsed2, TIMEOUT):
            log(f"âŒ {proxy_url} : too slow in functional check ({elapsed2:.2f}s)")
            return False

        # ã“ã“ã¾ã§ OK ãªã‚‰å®Ÿç”¨çš„ã«åˆ©ç”¨å¯èƒ½ã¨åˆ¤æ–­
        log(f"âœ… {proxy_url} ok (basic {elapsed:.2f}s, functional {elapsed2:.2f}s)")
        return True

    except Exception as e:
        # è©³ç´°ãªãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’æ®‹ã—ã¦ False ã‚’è¿”ã™
        log(f"âŒ {proxy_url} exception: {e}")
        return False


def check_list_parallel(proxy_list, my_ip):
    """
    ä¸¦åˆ—ãƒã‚§ãƒƒã‚¯ã‚’å …ç‰¢åŒ–: future.result() ã®ä¾‹å¤–ã‚’æ•ã¾ãˆã€å€‹åˆ¥ã®ãƒ—ãƒ­ã‚­ã‚·è©•ä¾¡ã«å¤±æ•—ã—ã¦ã‚‚å…¨ä½“ãŒæ­¢ã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚
    """
    if not proxy_list:
        return set()
    alive = set()
    total = len(proxy_list)
    completed = 0

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_proxy = {executor.submit(check_proxy, p, my_ip): p for p in proxy_list}
        for future in concurrent.futures.as_completed(future_to_proxy):
            completed += 1
            if completed % 500 == 0 or completed <= 5:
                print(f"   Progress: {completed}/{total} ...")
            proxy = future_to_proxy[future]
            try:
                result = future.result()
            except Exception as e:
                # å°†æ¥çš„ãªä¾‹å¤–ã«å‚™ãˆã¦ä¿é™ºçš„ã«æ•æ‰ï¼ˆé€šå¸¸ check_proxy ã¯ä¾‹å¤–ã‚’æ¡ã‚Šã¤ã¶ã™ï¼‰
                log(f"âŒ {proxy} future exception: {e}")
                result = False
            if result:
                alive.add(proxy)
    return alive


def main():
    log("ğŸš€ Proxy Checker (No Duplicates Mode)")
    my_ip = get_my_ip()
    if not my_ip: return

    prev_alive = load_prev_alive()
    prev_cache = load_file_as_set(FILE_CACHE)
    current_source = download_all_lists()
    
    if not current_source: current_source = prev_cache

    new_arrivals = current_source - prev_cache
    targets_new = new_arrivals - prev_alive
    
    log(f"ğŸ“‹ å†ãƒã‚§ãƒƒã‚¯: {len(prev_alive)}ä»¶")
    log(f"ğŸ“‹ æ–°è¦ãƒã‚§ãƒƒã‚¯: {len(targets_new)}ä»¶")

    alive_recheck = check_list_parallel(prev_alive, my_ip)
    alive_new = check_list_parallel(targets_new, my_ip)
    
    final_alive = alive_recheck | alive_new
    
    save_alive_split(final_alive)
    save_set_to_file(FILE_CACHE, current_source)
    
    log(f"âœ… å®Œäº†ã€‚å…¨ {len(final_alive)} ä»¶")

if __name__ == "__main__":
    main()