import requests
import os
import concurrent.futures
from datetime import datetime

# --- è¨­å®š ---
# ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã”ã¨ã®ãƒªã‚¹ãƒˆURL
SOURCES = {
    "http": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/http.txt",
    "socks4": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks4.txt",
    "socks5": "https://raw.githubusercontent.com/TheSpeedX/PROXY-List/master/socks5.txt"
}

FILE_ALIVE = "alive.txt"
FILE_CACHE = "list_cache.txt"
CHECK_URL = "http://httpbin.org/ip"
TIMEOUT = 10
MAX_WORKERS = 100 

def log(msg):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")

def get_my_ip():
    try:
        return requests.get(CHECK_URL, timeout=TIMEOUT).json()['origin']
    except:
        log("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼")
        return None

def download_all_lists():
    """å…¨ç¨®é¡ã®ãƒªã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä»˜ä¸ã—ã¦çµ±åˆã™ã‚‹"""
    combined_proxies = set()
    
    for protocol, url in SOURCES.items():
        log(f"ğŸ“¥ {protocol.upper()} ãƒªã‚¹ãƒˆã‚’å–å¾—ä¸­...")
        try:
            resp = requests.get(url, timeout=20)
            resp.raise_for_status()
            
            count = 0
            for line in resp.text.splitlines():
                p = line.strip()
                if p:
                    # IP:Port ã®å½¢å¼ãªã‚‰ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’é ­ã«ã¤ã‘ã‚‹
                    # (ä¾‹: socks5://1.1.1.1:1080)
                    if "://" not in p:
                        p = f"{protocol}://{p}"
                    combined_proxies.add(p)
                    count += 1
            log(f"   -> {count} ä»¶å–å¾—")
            
        except Exception as e:
            log(f"âŒ {protocol.upper()} å–å¾—å¤±æ•—: {e}")
    
    return combined_proxies

def load_file_as_set(filename):
    if not os.path.exists(filename):
        return set()
    with open(filename, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

def save_set_to_file(filename, data_set):
    with open(filename, "w", encoding="utf-8") as f:
        # ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã”ã¨ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¿å­˜ã™ã‚‹ã¨è¦‹ã‚„ã™ã„
        for item in sorted(list(data_set)):
            f.write(item + "\n")

def check_proxy(proxy_url, my_ip):
    """
    proxy_url ã¯ 'socks5://1.1.1.1:80' ã®ã‚ˆã†ãªå½¢å¼ã§æ¸¡ã£ã¦ãã‚‹
    requests[socks] ãŒå…¥ã£ã¦ã„ã‚Œã°ãã®ã¾ã¾ä½¿ãˆã‚‹
    """
    proxies = {"http": proxy_url, "https": proxy_url}
    try:
        resp = requests.get(CHECK_URL, proxies=proxies, timeout=TIMEOUT)
        resp.raise_for_status()
        if my_ip in resp.json()['origin']: return False
        return True
    except:
        return False

def check_list_parallel(proxy_list, my_ip):
    if not proxy_list: return set()
    alive = set()
    
    # å®Œäº†æ•°è¡¨ç¤ºç”¨
    total = len(proxy_list)
    completed = 0
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_proxy = {executor.submit(check_proxy, p, my_ip): p for p in proxy_list}
        
        for future in concurrent.futures.as_completed(future_to_proxy):
            completed += 1
            if completed % 500 == 0:
                print(f"   Progress: {completed}/{total} ...")
                
            if future.result():
                alive.add(future_to_proxy[future])
    return alive

def main():
    log("ğŸš€ GitHub Actions Proxy Checker (HTTP/SOCKS4/SOCKS5)")
    my_ip = get_my_ip()
    if not my_ip: return

    prev_alive = load_file_as_set(FILE_ALIVE)
    prev_cache = load_file_as_set(FILE_CACHE)
    
    # å…¨ã‚½ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦çµ±åˆ
    current_source = download_all_lists()
    
    if not current_source: 
        log("âš ï¸ ãƒªã‚¹ãƒˆãŒä¸€ã¤ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ—¢å­˜ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        current_source = prev_cache

    # å·®åˆ†è¨ˆç®—
    new_arrivals = current_source - prev_cache
    targets_new = new_arrivals - prev_alive
    
    log(f"ğŸ“‹ å†ãƒã‚§ãƒƒã‚¯: {len(prev_alive)}ä»¶")
    log(f"ğŸ“‹ æ–°è¦ãƒã‚§ãƒƒã‚¯: {len(targets_new)}ä»¶ (å‰å›ã¨ã®å·®åˆ†)")

    # ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
    alive_recheck = check_list_parallel(prev_alive, my_ip)
    alive_new = check_list_parallel(targets_new, my_ip)
    
    final_alive = alive_recheck | alive_new
    
    save_set_to_file(FILE_ALIVE, final_alive)
    save_set_to_file(FILE_CACHE, current_source)
    
    log(f"âœ… å®Œäº†: {len(final_alive)} ä»¶ãŒç”Ÿå­˜ã€‚")

if __name__ == "__main__":
    main()
